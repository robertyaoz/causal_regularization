{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as slin\n",
    "import scipy.optimize as sopt\n",
    "from scipy.special import expit as sigmoid\n",
    "import networkx as nx\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "def notears_linear_l1(X, lambda1, loss_type, max_iter=100, h_tol=1e-8, rho_max=1e+16, w_threshold=0.3):\n",
    "    \"\"\"Solve min_W L(W; X) + lambda1 ‖W‖_1 s.t. h(W) = 0 using augmented Lagrangian.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): [n, d] sample matrix\n",
    "        lambda1 (float): l1 penalty parameter\n",
    "        loss_type (str): l2, logistic, poisson\n",
    "        max_iter (int): max num of dual ascent steps\n",
    "        h_tol (float): exit if |h(w_est)| <= htol\n",
    "        rho_max (float): exit if rho >= rho_max\n",
    "        w_threshold (float): drop edge if |weight| < threshold\n",
    "\n",
    "    Returns:\n",
    "        W_est (np.ndarray): [d, d] estimated DAG\n",
    "    \"\"\"\n",
    "    def _loss(W):\n",
    "        \"\"\"Evaluate value and gradient of loss.\"\"\"\n",
    "        M = X @ W\n",
    "        if loss_type == 'l2':\n",
    "            R = X - M\n",
    "            loss = 0.5 / X.shape[0] * (R ** 2).sum()\n",
    "            G_loss = - 1.0 / X.shape[0] * X.T @ R\n",
    "        elif loss_type == 'logistic':\n",
    "            loss = 1.0 / X.shape[0] * (np.logaddexp(0, M) - X * M).sum()\n",
    "            G_loss = 1.0 / X.shape[0] * X.T @ (sigmoid(M) - X)\n",
    "        elif loss_type == 'poisson':\n",
    "            S = np.exp(M)\n",
    "            loss = 1.0 / X.shape[0] * (S - X * M).sum()\n",
    "            G_loss = 1.0 / X.shape[0] * X.T @ (S - X)\n",
    "        else:\n",
    "            raise ValueError('unknown loss type')\n",
    "        return loss, G_loss\n",
    "\n",
    "    def _h(W):\n",
    "        \"\"\"Evaluate value and gradient of acyclicity constraint.\"\"\"\n",
    "        #     E = slin.expm(W * W)  # (Zheng et al. 2018)\n",
    "        #     h = np.trace(E) - d\n",
    "        M = np.eye(d) + W * W / d  # (Yu et al. 2019)\n",
    "        E = np.linalg.matrix_power(M, d - 1)\n",
    "        h = (E.T * M).sum() - d\n",
    "        G_h = E.T * W * 2\n",
    "        return h, G_h\n",
    "\n",
    "    def _adj(w):\n",
    "        \"\"\"Convert doubled variables ([2 d^2] array) back to original variables ([d, d] matrix).\"\"\"\n",
    "        return (w[:d * d] - w[d * d:]).reshape([d, d])\n",
    "\n",
    "    def _func(w):\n",
    "        \"\"\"Evaluate value and gradient of augmented Lagrangian for doubled variables ([2 d^2] array).\"\"\"\n",
    "        W = _adj(w)\n",
    "        loss, G_loss = _loss(W)\n",
    "        h, G_h = _h(W)\n",
    "        # us obj as loss\n",
    "        obj = loss + 0.5 * rho * h * h + alpha * h + lambda1 * w.sum()\n",
    "        G_smooth = G_loss + (rho * h + alpha) * G_h\n",
    "        g_obj = np.concatenate((G_smooth + lambda1, - G_smooth + lambda1), axis=None)\n",
    "        return obj, g_obj\n",
    "\n",
    "    n, d = X.shape\n",
    "    w_est, rho, alpha, h = np.zeros(2 * d * d), 1.0, 0.0, np.inf  # double w_est into (w_pos, w_neg)\n",
    "    bnds = [(0, 0) if i == j else (0, None) for _ in range(2) for i in range(d) for j in range(d)]\n",
    "    for jj in range(max_iter):\n",
    "\n",
    "        WWW = _adj(w_est)\n",
    "        risk, _ = _loss(WWW)\n",
    "        print('Step',jj,'Loss', risk,'rho',rho,'alpha',alpha)\n",
    "        w_new, h_new = None, None\n",
    "        while rho < rho_max:\n",
    "            sol = sopt.minimize(_func, w_est, method='L-BFGS-B', jac=True, bounds=bnds)\n",
    "            w_new = sol.x\n",
    "            h_new, _ = _h(_adj(w_new))\n",
    "            if h_new > 0.25 * h:\n",
    "                rho *= 10\n",
    "            else:\n",
    "                break\n",
    "        w_est, h = w_new, h_new\n",
    "        alpha += rho * h\n",
    "        if h <= h_tol or rho >= rho_max:\n",
    "            break\n",
    "    #print(w_est)\n",
    "    W_est = _adj(w_est)\n",
    "    #W_est[np.abs(W_est) < w_threshold] = 0\n",
    "    return W_est\n",
    "\n",
    "\n",
    "\n",
    "def random_dag(nodes, edges):\n",
    "    \"\"\"Generate a random Directed Acyclic Graph (DAG) with a given number of nodes and edges.\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(nodes):\n",
    "        G.add_node(i)\n",
    "    while edges > 0:\n",
    "        a = random.randint(0,nodes-1)\n",
    "        b=a\n",
    "        while b==a:\n",
    "            b = random.randint(0,nodes-1)\n",
    "        G.add_edge(a,b)\n",
    "        if nx.is_directed_acyclic_graph(G):\n",
    "            edges -= 1\n",
    "        else:\n",
    "            # we closed a loop!\n",
    "            G.remove_edge(a,b)\n",
    "    return G\n",
    "\n",
    "# This function generates data according to a DAG provided in list_vertex and list_edges with mean and variance as input\n",
    "# It will apply a perturbation at each node provided in perturb.\n",
    "def gen_data(G, mean = 0, var = 1, SIZE = 10000, perturb = []):\n",
    "    list_edges = G.edges()\n",
    "    list_vertex = G.nodes()\n",
    "\n",
    "    order = []\n",
    "    for ts in nx.algorithms.dag.topological_sort(G):\n",
    "        order.append(ts)\n",
    "\n",
    "    g = []\n",
    "    for v in list_vertex:\n",
    "        if v in perturb:\n",
    "            g.append(np.random.normal(mean,var,SIZE))\n",
    "            print(\"perturbing \", v, \"with mean var = \", mean, var)\n",
    "        else:\n",
    "            g.append(np.random.normal(0,1,SIZE))\n",
    "\n",
    "    for o in order:\n",
    "        for edge in list_edges:\n",
    "            if o == edge[1]: # if there is an edge into this node\n",
    "                g[edge[1]] += g[edge[0]]\n",
    "    g = np.swapaxes(g,0,1)\n",
    "    return pd.DataFrame(g, columns = list(map(str, list_vertex)))\n",
    "\n",
    "\n",
    "nodes = 5\n",
    "datasize = 5000\n",
    "G = random_dag(nodes, nodes*nodes)\n",
    "df = gen_data(G, SIZE = datasize)\n",
    "\n",
    "X_DAG = df.values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size =32\n",
    "lambda1 = 0.1\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "\n",
    "num_inputs = 5\n",
    "n_hidden = 5\n",
    "num_outputs = 1 #regresion output\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_inputs])\n",
    "#Y = tf.placeholder(\"float\", [None, num_outputs])\n",
    "rho =  tf.placeholder(\"float\",[1,1])\n",
    "alpha =  tf.placeholder(\"float\",[1,1])\n",
    "\n",
    "\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'w1': tf.Variable(tf.random_normal([num_inputs,n_hidden]))\n",
    "}\n",
    "\n",
    "weights['w1'] = weights['w1'] - tf.matrix_diag(tf.linalg.diag_part(weights['w1']))\n",
    "W = weights['w1']\n",
    "M = X @ W\n",
    "\n",
    "R = X - M\n",
    "mse_loss = tf.reduce_mean(tf.reduce_sum(tf.square(R),axis=1),axis=0)\n",
    "\n",
    "d = tf.cast(X.shape[1], tf.float32)\n",
    "\n",
    "#M = tf.cast(tf.eye(tf.cast(d,tf.int32)),tf.float32) + W * W / d\n",
    "\n",
    "#E = M**(d-1)\n",
    "\n",
    "dag_l = tf.linalg.trace(tf.linalg.expm(tf.math.multiply(W,W)))\n",
    "\n",
    "#h= tf.abs(tf.reduce_sum(tf.transpose(E) * M)- d)\n",
    "\n",
    "h =  tf.abs(dag_l - d)\n",
    "\n",
    "'''Some NOTEARS params'''\n",
    "\n",
    "\n",
    "'''\n",
    "obj = loss + 0.5 * rho * h * h + alpha * h + lambda1 * w.sum()\n",
    "'''\n",
    "#alpha * h\n",
    "custom_loss = mse_loss + 0.5 * rho* h * h  + alpha * h +   lambda1 * tf.reduce_sum(tf.abs(weights['w1']))\n",
    "#return custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Total Loss= 1259.5887 rho: 1.0 alpha: 0.0\n",
      "Step 2, Total Loss= 353.7227 rho: 1.0 alpha: 7.966518402099609\n",
      "Step 3, Total Loss= 61.1074 rho: 1.0 alpha: 9.106798648834229\n",
      "Step 4, Total Loss= 5.0235 rho: 100.0 alpha: 19.911022663116455\n",
      "Step 5, Total Loss= 3.8441 rho: 1000.0 alpha: 42.04151201248169\n",
      "Step 6, Total Loss= 4.0749 rho: 10000.0 alpha: 89.65370225906372\n",
      "Step 7, Total Loss= 4.4943 rho: 100000.0 alpha: 208.5768895149231\n",
      "Step 8, Total Loss= 4.7391 rho: 1000000.0 alpha: 467.49946641921997\n",
      "Step 9, Total Loss= 4.8566 rho: 10000000.0 alpha: 1030.1673130989075\n",
      "Optimization Finished!\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Construct model\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "loss_op = optimizer.minimize(custom_loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "h_out = np.inf\n",
    "h_tol=1e-8 \n",
    "rho_max=1e+16\n",
    "w_threshold=0.3\n",
    "rho_i = np.array([[1.0]])\n",
    "alpha_i = np.array([[0.0]])\n",
    "\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, 10):\n",
    "        \n",
    "        _, loss = sess.run([loss_op, mse_loss], feed_dict={X: X_DAG, rho:rho_i, alpha:alpha_i})\n",
    "        print(\"Step \" + str(step) + \", Total Loss= \" + \"{:.4f}\".format(loss),\"rho:\", rho_i[0,0],\"alpha:\",alpha_i[0,0]) \n",
    "        \n",
    "        \n",
    "        while rho_i < rho_max:\n",
    "\n",
    "            for step1 in range(1, 2000):\n",
    "                batch_x = np.array(random.choices(X_DAG, k=batch_size))\n",
    "\n",
    "\n",
    "                sess.run(loss_op, feed_dict={X: X_DAG, rho:rho_i,alpha:alpha_i})\n",
    "\n",
    "\n",
    "            h_new,w_new = sess.run([h,weights['w1']], feed_dict={X: X_DAG, rho:rho_i, alpha:alpha_i})\n",
    "            if h_new > 0.25 * h_out:\n",
    "                rho_i *= 10\n",
    "            else:\n",
    "                break\n",
    "        h_out, W_est = h_new, w_new\n",
    "        alpha_i += rho_i * h_out\n",
    "        if h_out <= h_tol or rho_i >= rho_max:\n",
    "            break\n",
    "\n",
    "    W_est = sess.run(weights['w1'], feed_dict={X: X_DAG, rho:rho_i, alpha:alpha_i})\n",
    "    print(\"Optimization Finished!\")\n",
    "    W_est[np.abs(W_est) < w_threshold] = 0\n",
    "    print((np.round_(W_est,decimals=3)>0)*1.0)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 Loss 59.00077055560621 rho 1.0 alpha 0.0\n",
      "Step 1 Loss 1.21510456252768 rho 1.0 alpha 0.3013669315036074\n",
      "Step 2 Loss 1.415088005758514 rho 100.0 alpha 5.62450549845451\n",
      "Step 3 Loss 1.9853708990196517 rho 10000.0 alpha 52.26086902787209\n",
      "Step 4 Loss 2.2482685819082193 rho 100000.0 alpha 157.75780960476004\n",
      "Step 5 Loss 2.4480257123272384 rho 1000000.0 alpha 418.19056444909086\n",
      "Step 6 Loss 2.460903127657992 rho 10000000.0 alpha 877.2051788292285\n",
      "Step 7 Loss 2.491211958635835 rho 100000000.0 alpha 1875.3458763904403\n",
      "Step 8 Loss 2.505221244554227 rho 1000000000.0 alpha 4029.366885949855\n",
      "Step 9 Loss 2.5147235762660736 rho 10000000000.0 alpha 8715.228318157471\n",
      "Step 10 Loss 2.5191960164481166 rho 100000000000.0 alpha 18830.2299544369\n",
      "Step 11 Loss 2.5212814279251874 rho 1000000000000.0 alpha 40623.39633705898\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "W_est = notears_linear_l1(X_DAG, lambda1=0.1, loss_type='l2')\n",
    "\n",
    "W_est[np.abs(W_est) < w_threshold] = 0\n",
    "print((np.round_(W_est,decimals=3)>0)*1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0]\n",
      " [1 0 1 1 1]\n",
      " [1 0 0 1 1]\n",
      " [0 0 0 0 0]\n",
      " [1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(nx.adjacency_matrix(G).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:yaoaim]",
   "language": "python",
   "name": "conda-env-yaoaim-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
