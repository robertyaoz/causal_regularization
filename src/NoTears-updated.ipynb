{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as slin\n",
    "import scipy.optimize as sopt\n",
    "from scipy.special import expit as sigmoid\n",
    "import networkx as nx\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "def notears_linear_l1(X, lambda1, loss_type, max_iter=100, h_tol=1e-8, rho_max=1e+16, w_threshold=0.3):\n",
    "    \"\"\"Solve min_W L(W; X) + lambda1 ‖W‖_1 s.t. h(W) = 0 using augmented Lagrangian.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): [n, d] sample matrix\n",
    "        lambda1 (float): l1 penalty parameter\n",
    "        loss_type (str): l2, logistic, poisson\n",
    "        max_iter (int): max num of dual ascent steps\n",
    "        h_tol (float): exit if |h(w_est)| <= htol\n",
    "        rho_max (float): exit if rho >= rho_max\n",
    "        w_threshold (float): drop edge if |weight| < threshold\n",
    "\n",
    "    Returns:\n",
    "        W_est (np.ndarray): [d, d] estimated DAG\n",
    "    \"\"\"\n",
    "    def _loss(W):\n",
    "        \"\"\"Evaluate value and gradient of loss.\"\"\"\n",
    "        M = X @ W\n",
    "        if loss_type == 'l2':\n",
    "            R = X - M\n",
    "            loss = 0.5 / X.shape[0] * (R ** 2).sum()\n",
    "            G_loss = - 1.0 / X.shape[0] * X.T @ R\n",
    "        elif loss_type == 'logistic':\n",
    "            loss = 1.0 / X.shape[0] * (np.logaddexp(0, M) - X * M).sum()\n",
    "            G_loss = 1.0 / X.shape[0] * X.T @ (sigmoid(M) - X)\n",
    "        elif loss_type == 'poisson':\n",
    "            S = np.exp(M)\n",
    "            loss = 1.0 / X.shape[0] * (S - X * M).sum()\n",
    "            G_loss = 1.0 / X.shape[0] * X.T @ (S - X)\n",
    "        else:\n",
    "            raise ValueError('unknown loss type')\n",
    "        return loss, G_loss\n",
    "\n",
    "    def _h(W):\n",
    "        \"\"\"Evaluate value and gradient of acyclicity constraint.\"\"\"\n",
    "        #     E = slin.expm(W * W)  # (Zheng et al. 2018)\n",
    "        #     h = np.trace(E) - d\n",
    "        M = np.eye(d) + W * W / d  # (Yu et al. 2019)\n",
    "        E = np.linalg.matrix_power(M, d - 1)\n",
    "        h = (E.T * M).sum() - d\n",
    "        G_h = E.T * W * 2\n",
    "        return h, G_h\n",
    "\n",
    "    def _adj(w):\n",
    "        \"\"\"Convert doubled variables ([2 d^2] array) back to original variables ([d, d] matrix).\"\"\"\n",
    "        return (w[:d * d] - w[d * d:]).reshape([d, d])\n",
    "\n",
    "    def _func(w):\n",
    "        \"\"\"Evaluate value and gradient of augmented Lagrangian for doubled variables ([2 d^2] array).\"\"\"\n",
    "        W = _adj(w)\n",
    "        loss, G_loss = _loss(W)\n",
    "        h, G_h = _h(W)\n",
    "        # us obj as loss\n",
    "        obj = loss + 0.5 * rho * h * h + alpha * h + lambda1 * w.sum()\n",
    "        G_smooth = G_loss + (rho * h + alpha) * G_h\n",
    "        g_obj = np.concatenate((G_smooth + lambda1, - G_smooth + lambda1), axis=None)\n",
    "        return obj, g_obj\n",
    "\n",
    "    n, d = X.shape\n",
    "    w_est, rho, alpha, h = np.zeros(2 * d * d), 1.0, 0.0, np.inf  # double w_est into (w_pos, w_neg)\n",
    "    bnds = [(0, 0) if i == j else (0, None) for _ in range(2) for i in range(d) for j in range(d)]\n",
    "    for _ in range(max_iter):\n",
    "        w_new, h_new = None, None\n",
    "        while rho < rho_max:\n",
    "            sol = sopt.minimize(_func, w_est, method='L-BFGS-B', jac=True, bounds=bnds)\n",
    "            w_new = sol.x\n",
    "            h_new, _ = _h(_adj(w_new))\n",
    "            if h_new > 0.25 * h:\n",
    "                rho *= 10\n",
    "            else:\n",
    "                break\n",
    "        w_est, h = w_new, h_new\n",
    "        alpha += rho * h\n",
    "        if h <= h_tol or rho >= rho_max:\n",
    "            break\n",
    "    print(w_est)\n",
    "    W_est = _adj(w_est)\n",
    "    W_est[np.abs(W_est) < w_threshold] = 0\n",
    "    return W_est\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_dag(nodes, edges):\n",
    "    \"\"\"Generate a random Directed Acyclic Graph (DAG) with a given number of nodes and edges.\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(nodes):\n",
    "        G.add_node(i)\n",
    "    while edges > 0:\n",
    "        a = random.randint(0,nodes-1)\n",
    "        b=a\n",
    "        while b==a:\n",
    "            b = random.randint(0,nodes-1)\n",
    "        G.add_edge(a,b)\n",
    "        if nx.is_directed_acyclic_graph(G):\n",
    "            edges -= 1\n",
    "        else:\n",
    "            # we closed a loop!\n",
    "            G.remove_edge(a,b)\n",
    "    return G\n",
    "\n",
    "# This function generates data according to a DAG provided in list_vertex and list_edges with mean and variance as input\n",
    "# It will apply a perturbation at each node provided in perturb.\n",
    "def gen_data(G, mean = 0, var = 1, SIZE = 10000, perturb = []):\n",
    "    list_edges = G.edges()\n",
    "    list_vertex = G.nodes()\n",
    "\n",
    "    order = []\n",
    "    for ts in nx.algorithms.dag.topological_sort(G):\n",
    "        order.append(ts)\n",
    "\n",
    "    g = []\n",
    "    for v in list_vertex:\n",
    "        if v in perturb:\n",
    "            g.append(np.random.normal(mean,var,SIZE))\n",
    "            print(\"perturbing \", v, \"with mean var = \", mean, var)\n",
    "        else:\n",
    "            g.append(np.random.normal(0,1,SIZE))\n",
    "\n",
    "    for o in order:\n",
    "        for edge in list_edges:\n",
    "            if o == edge[1]: # if there is an edge into this node\n",
    "                g[edge[1]] += g[edge[0]]\n",
    "    g = np.swapaxes(g,0,1)\n",
    "    return pd.DataFrame(g, columns = list(map(str, list_vertex)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 5\n",
    "datasize = 1000\n",
    "G = random_dag(nodes, nodes*nodes)\n",
    "df = gen_data(G, SIZE = datasize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 9.29348164e-05 5.86389088e-06 2.01444849e-05\n",
      " 4.82854217e-05 4.98242295e-01 0.00000000e+00 7.93205218e-01\n",
      " 9.13842392e-01 8.87965036e-01 1.24654892e+00 3.71878347e-05\n",
      " 0.00000000e+00 6.75522626e-06 1.75611865e-05 9.07470025e-01\n",
      " 1.44498862e-05 1.10635903e+00 0.00000000e+00 5.05883693e-06\n",
      " 6.03671023e-01 4.46866466e-06 8.87735553e-01 9.98802459e-01\n",
      " 0.00000000e+00 0.00000000e+00 4.03214599e-05 1.85253693e-06\n",
      " 5.10222327e-06 1.51515475e-05 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 9.43339705e-06 0.00000000e+00 1.38332912e-06 4.03629591e-06\n",
      " 0.00000000e+00 3.50783548e-06 0.00000000e+00 0.00000000e+00\n",
      " 1.26140294e-06 0.00000000e+00 1.67897105e-06 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.49824229, 0.        , 0.79320522, 0.91384239, 0.88796504],\n",
       "       [1.24654892, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.90747003, 0.        , 1.10635903, 0.        , 0.        ],\n",
       "       [0.60367102, 0.        , 0.88773555, 0.99880246, 0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_DAG = df.values\n",
    "W_est = notears_linear_l1(X_DAG, lambda1=0.1, loss_type='l2')\n",
    "W_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Stuff for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 2500\n",
    "batch_size = 32\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "\n",
    "num_inputs = 5\n",
    "n_hidden = 5\n",
    "num_outputs = 1 #regresion output\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_inputs])\n",
    "Y = tf.placeholder(\"float\", [None, num_outputs])\n",
    "\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_inputs, n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, num_outputs]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([num_outputs]))\n",
    "}\n",
    "\n",
    "def neural_net(x):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "def regularization_loss(X):\n",
    "    W = weights['h1'] # YAO: I am unclear if we want the edge weights h1 or out...\n",
    "    '''\n",
    "    M = X @ W\n",
    "    if loss_type == 'l2':\n",
    "        R = X - M\n",
    "        loss = 0.5 / X.shape[0] * (R ** 2).sum()\n",
    "        G_loss = - 1.0 / X.shape[0] * X.T @ R\n",
    "    '''\n",
    "    M = X @ W\n",
    "    R = X - M\n",
    "    mse_loss = tf.reduce_mean(tf.square(R))\n",
    "    '''\n",
    "    M = np.eye(d) + W * W / d  # (Yu et al. 2019)\n",
    "    E = np.linalg.matrix_power(M, d - 1)\n",
    "    h = (E.T * M).sum() - d\n",
    "    '''\n",
    "    n, d = X.shape\n",
    "    M = tf.convert_to_tensor(np.eye(d))\n",
    "    d = tf.cast(d, tf.float32)\n",
    "    M = M + tf.cast(W * W / d, tf.float64)\n",
    "    E = tf.pow(M, tf.cast(d - 1, tf.float64))\n",
    "\n",
    "    h = tf.reduce_sum(tf.cast(tf.transpose(E) * M, tf.float32)) - d \n",
    "\n",
    "    '''Some NOTEARS params'''\n",
    "    rho = 1 # I don't think we need to do anything with this.\n",
    "    alpha = 1 #same here\n",
    "    lambda1 = 0.1 \n",
    "\n",
    "    '''\n",
    "    obj = loss + 0.5 * rho * h * h + alpha * h + lambda1 * w.sum()\n",
    "    '''\n",
    "    custom_loss = mse_loss + 0.5 * h * h * rho + alpha * h + lambda1 * tf.reduce_sum(W) \n",
    "    return custom_loss\n",
    "\n",
    "\n",
    "# Construct model\n",
    "model = neural_net(X)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "loss_op = optimizer.minimize(regularization_loss(X))\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 211.3484\n",
      "Step 100, Minibatch Loss= 168.0577\n",
      "Step 200, Minibatch Loss= 260.8247\n",
      "Step 300, Minibatch Loss= 121.1237\n",
      "Step 400, Minibatch Loss= 85.0117\n",
      "Step 500, Minibatch Loss= 66.0043\n",
      "Step 600, Minibatch Loss= 55.5152\n",
      "Step 700, Minibatch Loss= 67.8782\n",
      "Step 800, Minibatch Loss= 40.3970\n",
      "Step 900, Minibatch Loss= 22.0684\n",
      "Step 1000, Minibatch Loss= 31.6012\n",
      "Step 1100, Minibatch Loss= 22.7454\n",
      "Step 1200, Minibatch Loss= 21.1569\n",
      "Step 1300, Minibatch Loss= 14.8793\n",
      "Step 1400, Minibatch Loss= 12.5130\n",
      "Step 1500, Minibatch Loss= 11.9307\n",
      "Step 1600, Minibatch Loss= 7.0512\n",
      "Step 1700, Minibatch Loss= 6.7492\n",
      "Step 1800, Minibatch Loss= 5.6420\n",
      "Step 1900, Minibatch Loss= 5.7373\n",
      "Step 2000, Minibatch Loss= 4.5596\n",
      "Step 2100, Minibatch Loss= 4.3620\n",
      "Step 2200, Minibatch Loss= 4.3702\n",
      "Step 2300, Minibatch Loss= 3.3913\n",
      "Step 2400, Minibatch Loss= 3.1503\n",
      "Step 2500, Minibatch Loss= 2.8304\n",
      "Optimization Finished!\n",
      "[[ 0.4915184  -0.26115128 -0.25499156  0.60967195  0.9604991 ]\n",
      " [ 0.54276454  0.02792393  0.8928426  -0.10516954 -0.29307058]\n",
      " [ 0.08734593  0.270296    0.45998874 -0.27904007 -1.3060632 ]\n",
      " [ 0.9175119   0.08979382  0.4885157   0.2312842  -0.07468524]\n",
      " [ 1.1375868   1.356341    1.8070427  -1.9998281  -0.25186697]]\n",
      "[[0 0 0 0 0]\n",
      " [1 0 1 1 1]\n",
      " [1 0 0 0 0]\n",
      " [1 0 1 0 0]\n",
      " [1 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x = np.array(random.choices(X_DAG, k=batch_size))\n",
    "        batch_y = np.zeros((32, 1)) # For now this 'Y does nothing'\n",
    "        sess.run(loss_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            _, loss = sess.run([loss_op, regularization_loss(X)], feed_dict={X: batch_x, Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \"{:.4f}\".format(loss)) \n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    print(weights['h1'].eval())\n",
    "    print(nx.adjacency_matrix(G).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
